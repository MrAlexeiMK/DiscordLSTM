{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\пк\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_MAX_WORDS = 20\n",
    "EXTRACT_MAX = 200000\n",
    "WEIGHTS_LOCATION = \"weights/model2/\"\n",
    "OUTPUT_FILE = \"data/discord_conversation_chopy.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization (words selection), deleting stop-words, lemmatization, stemming settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP:\n",
    "    @staticmethod\n",
    "    def extract_tokens(text):\n",
    "        tokens = [word.lower() for sent in sent_tokenize(text) for word in word_tokenize(sent)]\n",
    "        return tokens\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_sentences(text):\n",
    "        return sent_tokenize(text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_sentences_normalized(text):\n",
    "        sentences = NLP.extract_sentences(text)\n",
    "        for i in range(len(sentences)):\n",
    "            tokens = NLP.extract_tokens(sentences[i])\n",
    "            sentences[i] = ' '.join(tokens)\n",
    "        return sentences\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_words(array):\n",
    "        result = array.copy()\n",
    "        for i in range(len(result)):\n",
    "            text = re.sub('[^\\w$ ]', '', result[i])\n",
    "            try:\n",
    "                result[i] = NLP.extract_sentences_normalized(text)[0]\n",
    "            except:\n",
    "                result[i] = None\n",
    "        return np.array(result)\n",
    "    \n",
    "# Testing\n",
    "print(NLP.normalize_words([\"Привет! Как дела? Что думаешь о погоде???\", \n",
    "                           \"Хорошая погода...\", \n",
    "                           \"Yep! It's right!\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display 'discord_conversation.csv' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(OUTPUT_FILE, delimiter='$')\n",
    "df = df.dropna(how='any',axis=0) \n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation from 'discord_conversation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.to_numpy()\n",
    "pred_questions = NLP.normalize_words(dataset[:, 0].tolist())\n",
    "pred_answers = NLP.normalize_words([s + \" $\" for s in dataset[:, 1].tolist()])\n",
    "print(pred_questions)\n",
    "print(pred_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract 'question' -> 'answer word' data from each sentences of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for i in range(len(pred_questions)):\n",
    "    pq = pred_questions[i]\n",
    "    pa = pred_answers[i]\n",
    "    if pq is None or pa is None:\n",
    "        continue\n",
    "    \n",
    "    words = pa.split()\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        q = f\"{pq} {' '.join(words[:i])}\"\n",
    "        a = words[i]\n",
    "        questions.append(q)\n",
    "        answers.append(a)\n",
    "\n",
    "for i in range(min(30, len(questions))):\n",
    "    print(f\"[{questions[i]}] -> [{answers[i]}]\")\n",
    "\n",
    "questions, answers = shuffle(questions, answers)\n",
    "\n",
    "questions = questions[:EXTRACT_MAX]\n",
    "answers = answers[:EXTRACT_MAX]\n",
    "\n",
    "print(f\"\\nExtracted: {len(questions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 'question' -> 'answer' model on different methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (Long Short-Term Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model contains 18849 words in dictionary\n",
      "\n",
      "Dataset size is: 59557\n",
      "Questions shape: (59557, 20)\n",
      "Answers shape: (59557, 1)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters='!\"#%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(questions + answers)\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items())) #reverse map to get predicted word\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1 # '+ 1' for unknown words\n",
    "data_size = len(questions)\n",
    "\n",
    "print(f\"Model contains {vocab_size} words in dictionary\")\n",
    "\n",
    "#Get numbers sequences by tokenizer\n",
    "Q_temp = tokenizer.texts_to_sequences(questions)\n",
    "A_temp = tokenizer.texts_to_sequences(answers)\n",
    "Q = pad_sequences(Q_temp, padding='post', maxlen=QUESTION_MAX_WORDS)\n",
    "A = pad_sequences(A_temp, padding='post', maxlen=1)\n",
    "\n",
    "print(f\"\\nDataset size is: {data_size}\")\n",
    "print(f\"Questions shape: {Q.shape}\")\n",
    "print(f\"Answers shape: {A.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model load from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 20, 64)            1206336   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 18849)             1225185   \n",
      "=================================================================\n",
      "Total params: 2,464,545\n",
      "Trainable params: 2,464,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Loaded model contains 18848 words\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(WEIGHTS_LOCATION)\n",
    "model.summary()\n",
    "\n",
    "with open(WEIGHTS_LOCATION + 'dict.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "print(\"Loaded model contains \" + str(vocab_size) + \" words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical answers shape: (59557, 18849)\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 20, 64)            1206336   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 18849)             1225185   \n",
      "=================================================================\n",
      "Total params: 2,464,545\n",
      "Trainable params: 2,464,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # [26, 125, 1, ..., 0, 0, 0] * QUESTION_MAX_WORDS\n",
    "    tf.keras.layers.Embedding(vocab_size, 64, input_length=QUESTION_MAX_WORDS),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    # [0.1, 0.15, 0.35, ..., 0.02, 0.13] * vocab_size\n",
    "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "A = to_categorical(A, num_classes=vocab_size)\n",
    "print(f\"Categorical answers shape: {A.shape}\\n\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1676/1676 [==============================] - 77s 46ms/step - loss: 1.3311 - accuracy: 0.7314 - val_loss: 12.6847 - val_accuracy: 0.0573\n",
      "Epoch 2/10\n",
      "1676/1676 [==============================] - 93s 55ms/step - loss: 1.2989 - accuracy: 0.7368 - val_loss: 12.6493 - val_accuracy: 0.0579\n",
      "Epoch 3/10\n",
      "1269/1676 [=====================>........] - ETA: 23s - loss: 1.2282 - accuracy: 0.7513"
     ]
    }
   ],
   "source": [
    "model.fit(Q, A, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: weights/model2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: weights/model2/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(WEIGHTS_LOCATION)\n",
    "with open(WEIGHTS_LOCATION + 'dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input text and the desired number of words in the output\n",
    "input_text = \"Тест\"\n",
    "num_words = 50\n",
    "max_repeats = 3\n",
    "\n",
    "output_text = input_text\n",
    "current_repeats = 1\n",
    "last_word = None\n",
    "\n",
    "# Loop until the desired number of words is reached\n",
    "while num_words > 0:\n",
    "    # Encode the input text and pad it to the desired length\n",
    "    encoded_input = pad_sequences(tokenizer.texts_to_sequences(\n",
    "        NLP.extract_sentences_normalized(output_text)), padding='post', maxlen=QUESTION_MAX_WORDS)\n",
    "\n",
    "    # Use the model to predict the probability distribution over the next word\n",
    "    probs_output = model.predict(encoded_input)[0]\n",
    "\n",
    "    # Choose the most likely word (excluding the padding token)\n",
    "    index = np.argmax(probs_output[1:]) + 1\n",
    "\n",
    "    # If a valid word was predicted, append it to the output text\n",
    "    if index != 0:\n",
    "        word = reverse_word_map[index]\n",
    "        if word == \"$\":\n",
    "            break\n",
    "\n",
    "        if last_word != None:\n",
    "            if last_word == word:\n",
    "                current_repeats += 1\n",
    "            else:\n",
    "                current_repeats = 1\n",
    "                \n",
    "        if current_repeats > max_repeats:\n",
    "            break\n",
    "\n",
    "        output_text += \" \" + word\n",
    "        last_word = word\n",
    "\n",
    "    # Decrement the word counter\n",
    "    num_words -= 1\n",
    "\n",
    "# Print the output text\n",
    "print(\"Входной текст:\", input_text)\n",
    "print(\"Выходной текст:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
